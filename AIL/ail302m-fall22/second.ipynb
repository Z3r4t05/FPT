{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m set_config\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrandom\u001b[39;00m \u001b[39mimport\u001b[39;00m seed\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend \u001b[39mas\u001b[39;00m K, callbacks, layers\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_absolute_error, mean_squared_error\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:469\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_current_module, \u001b[39m\"\u001b[39m\u001b[39mkeras\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    468\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 469\u001b[0m     _keras\u001b[39m.\u001b[39;49m_load()\n\u001b[0;32m    470\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m    471\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_name] \u001b[39m=\u001b[39m module\n\u001b[0;32m     44\u001b[0m \u001b[39m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m input_spec\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m node \u001b[39mas\u001b[39;00m node_module\n\u001b[1;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m training \u001b[39mas\u001b[39;00m training_lib\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m training_utils\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaved_model\u001b[39;00m \u001b[39mimport\u001b[39;00m json_utils\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer_utils\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_utils\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m data_adapter\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m input_layer \u001b[39mas\u001b[39;00m input_layer_module\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m losses \u001b[39mas\u001b[39;00m losses_mod\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics \u001b[39mas\u001b[39;00m metrics_mod\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m saving_lib\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m generic_utils\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics\\__init__.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_metric\u001b[39;00m \u001b[39mimport\u001b[39;00m clone_metrics\n\u001b[0;32m     31\u001b[0m \u001b[39m# Metric functions\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m# Individual metric classes\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m AUC\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m Accuracy\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m BinaryAccuracy\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics\\metrics.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m utils \u001b[39mas\u001b[39;00m dtensor_utils\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\activations.py:21\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivation\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mactivation_layers\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m generic_utils\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\__init__.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Layer\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_preprocessing_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m PreprocessingLayer\n\u001b[0;32m     22\u001b[0m \u001b[39m# Generic layers.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:21\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mabc\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m data_adapter\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Layer\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m version_utils\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m keras_export\n\u001b[0;32m     42\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     pd \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\__init__.py:138\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    121\u001b[0m     concat,\n\u001b[0;32m    122\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m     qcut,\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[1;32m--> 138\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m testing  \u001b[39m# noqa:PDF015\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_print_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m    141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    142\u001b[0m     \u001b[39m# excel\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     ExcelFile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m     read_spss,\n\u001b[0;32m    172\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\testing.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mPublic testing utility functions.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_testing\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     assert_extension_array_equal,\n\u001b[0;32m      8\u001b[0m     assert_frame_equal,\n\u001b[0;32m      9\u001b[0m     assert_index_equal,\n\u001b[0;32m     10\u001b[0m     assert_series_equal,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_extension_array_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_frame_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_series_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_index_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_testing\\__init__.py:903\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpytest\u001b[39;00m\n\u001b[0;32m    900\u001b[0m     \u001b[39mreturn\u001b[39;00m pytest\u001b[39m.\u001b[39mraises(expected_exception, match\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)  \u001b[39m# noqa: PDF010\u001b[39;00m\n\u001b[1;32m--> 903\u001b[0m cython_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39mcommon\u001b[39m.\u001b[39m_cython_table\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    906\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[0;32m    907\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[39m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[39m    keys and expected result.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[39m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from sklearn import set_config\n",
    "from numpy.random import seed\n",
    "from tensorflow import keras\n",
    "from keras import backend as K, callbacks, layers\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"Tensorflow version: \" + tf.__version__)\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=14, titlepad=10)\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "seed(42)\n",
    "tf.random.set_seed(42)\n",
    "set_config(display='diagram')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input/train.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_plots(df, variable,target):\n",
    "    # The function takes a dataframe (df) and\n",
    "    # the variable of interest as arguments.\n",
    "\n",
    "    # Define figure size.\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 4, 1)\n",
    "    sns.histplot(df[variable], bins=30,color = 'r')\n",
    "    plt.title('Histogram')\n",
    "\n",
    "\n",
    "    # scatterplot\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.scatter(df[variable],df[target],color = 'g')\n",
    "    plt.title('Scatterplot')\n",
    "    \n",
    "    \n",
    "    # boxplot\n",
    "    plt.subplot(1, 4, 3)\n",
    "    sns.boxplot(y=df[variable],color = 'b')\n",
    "    plt.title('Boxplot')\n",
    "    \n",
    "    # barplot\n",
    "    plt.subplot(1, 4, 4)\n",
    "    sns.barplot(x = target, y = variable, data = df)   \n",
    "    plt.title('Barplot')\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "for variable in df.drop('type', axis = 1):\n",
    "    diagnostic_plots(df,variable,'quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.kurtosis(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='quality', col = 'type', kind='count', data=df, palette = 'magma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "sns.set(font_scale=1.25)\n",
    "sns.heatmap(df.corr(), cmap = 'YlGnBu', cbar_kws = {'shrink': 1}, annot = True, cbar = True, fmt = '.2f', annot_kws={'size': 10}, square = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ColumnTransformer and Pipeline objects are simple transformer so they can be put inside each other. Thus can be loop further as neeeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code is creating a pipeline for each of the transformers.\n",
    "transformer_cat = make_pipeline(OneHotEncoder(handle_unknown='ignore'))\n",
    "transformer_minmax = make_pipeline(MinMaxScaler(feature_range=(0, 1)))\n",
    "transformer_std = make_pipeline(StandardScaler())\n",
    "transformer_yeoj = make_pipeline(PowerTransformer(method='yeo-johnson', standardize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''features = df.columns.get_indexer(df.drop('type', axis = 1).columns.values).tolist()\n",
    "features_cat = df.columns.get_indexer(['type']).tolist()\n",
    "features_minmax = df.drop(['type', 'quality'], axis = 1).columns.values.tolist()\n",
    "features_std = df.columns.get_indexer(['citric acid', 'total sulfur dioxide', 'density', 'pH']).tolist()\n",
    "# get the index of the columns type and quality\n",
    "a = df.columns.get_indexer(['type', 'quality'])\n",
    "# get the index of the columns that are in both a and features std\n",
    "b = np.concatenate((features_std, a))\n",
    "# change ndarray to list\n",
    "c = b.tolist()\n",
    "# get the index of the columns that are not in c\n",
    "d = df.drop(df.columns[c], axis = 1).columns.values\n",
    "features_yeoj = df.columns.get_indexer(d).tolist()\n",
    "features_minmax = df.columns.get_indexer(features_minmax).tolist()'''\n",
    "features = list(df.drop('type', axis = 1).columns.values)\n",
    "features_cat = ['type']\n",
    "features_std = ['citric acid', 'total sulfur dioxide', 'density', 'pH']\n",
    "features_yeoj = ['fixed acidity', 'volatile acidity', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'sulphates', 'alcohol']\n",
    "features_minmax = df.drop(['type', 'quality'], axis = 1).columns.values.tolist()\n",
    "print(features)\n",
    "print(features_cat)\n",
    "print(features_minmax)\n",
    "print(features_std)\n",
    "print(features_yeoj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline that first transforms the data using the cols_transformer, then scales the data using the minmax scaler.\n",
    "cat = make_column_transformer(\n",
    "    (transformer_cat, features_cat),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "std = make_column_transformer(\n",
    "    (transformer_std, features_std),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "yeoj = make_column_transformer(\n",
    "    (transformer_yeoj, features_yeoj),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "minmax = make_column_transformer(\n",
    "    (transformer_minmax, [i for i in range(10)]),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "cols_transformer = make_column_transformer(\n",
    "    (transformer_yeoj, features_yeoj),\n",
    "    (transformer_std, features_std), \n",
    "    (transformer_cat, features_cat)\n",
    ")\n",
    "\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        self.shape = X.shape\n",
    "        # what other output you want\n",
    "        print(X[0])\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "pipe = Pipeline(\n",
    "    steps = [\n",
    "        (\"cols transformer\", cols_transformer),\n",
    "        (\"min max scaler\", minmax),\n",
    "    ]\n",
    ")\n",
    "\n",
    "'''12 0 1 2 3 4 5 6 7 8 9 10 11\n",
    "2 6 7 8 12 0 1 3 4 5 9 10 11\n",
    "0 1 3 4 5 9 10 11 2 6 7 8 12 \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(pipe.fit_transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = features_std + features_yeoj + ['red', 'white']\n",
    "quality = df['quality']\n",
    "# rearrange columns and add quality and type columns to the dataframe\n",
    "data = pd.concat([data[df.drop(['type', 'quality'], axis = 1).columns.values.tolist()], \n",
    "                  quality, data.iloc[:,11:]], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in data.drop(['red', 'white'], axis = 1):\n",
    "    diagnostic_plots(data,variable,'quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.skew(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.kurtosis(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data):\n",
    "    \"\"\"_summary_ : Split the data into train, validation and test sets.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Data to split\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: train, validation and test sets  \n",
    "    \"\"\"\n",
    "    train, val_and_test = train_test_split(data, test_size = 0.2, random_state=42)\n",
    "    val, test = train_test_split(val_and_test, test_size = 0.5, random_state=42)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_target(data):\n",
    "    \"\"\"_summary_ : Splitting the data into features and target.\n",
    "\n",
    "    Args:\n",
    "        data (_type_): data to split\n",
    "\n",
    "    Returns:\n",
    "        _type_: features and target\n",
    "    \"\"\"\n",
    "    # Splitting the data into features and target.\n",
    "    X, y = data.drop('quality', axis = 1), data['quality']\n",
    "    return X, y\n",
    "\n",
    "def transform_features(X):\n",
    "    \"\"\"_summary_ : Transforming the features using the preprocessor.\n",
    "\n",
    "    Args:\n",
    "        X (_type_): features to transform\n",
    "\n",
    "    Returns:\n",
    "        _type_: transformed features\n",
    "    \"\"\"\n",
    "    X_transform = pd.DataFrame(pipe.fit_transform(X))\n",
    "    X_transform.columns = features_minmax + ['red', 'white']\n",
    "    return X_transform \n",
    "\n",
    "def split_transform(data):\n",
    "    '''Splitting the data into features and target, and then transforming the features.'''\n",
    "    X, y = split_target(data)\n",
    "    return transform_features(X), y\n",
    "\n",
    "X_train, y_train = split_transform(train)\n",
    "X_val, y_val = split_transform(val)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (\n",
    "    Dense, Dropout, BatchNormalization\n",
    ")\n",
    "def build_model(hp):\n",
    "    \"\"\"_summary_ : Building a model using the hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        hp (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Creating a placeholder for the input data.\n",
    "    inputs = keras.Input(shape=(13,))\n",
    "    # Creating a dense layer with the parameters specified in the function.\n",
    "    x = Dense(units = hp.Int('units', min_value = 256, max_value = 1024, step = 32), activation = 'relu')(inputs)\n",
    "    x = Dense(units = hp.Int('units', min_value = 256, max_value = 1024, step = 32), activation = 'relu')(x)\n",
    "    x = Dropout(hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units = hp.Int('units', min_value = 256, max_value = 1024, step = 32), activation = 'relu')(x)\n",
    "    x = Dropout(hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units = hp.Int('units', min_value = 256, max_value = 1024, step = 32), activation = 'relu')(x)\n",
    "    x = Dropout(hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units = hp.Int('units', min_value = 256, max_value = 1024, step = 32), activation = 'relu')(x)\n",
    "    x = Dropout(hp.Float('dropout', min_value = 0.0, max_value = 0.5, step = 0.1))(x)                                                          \n",
    "    # Creating a dense layer with 1 unit.\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    # Creating a model with the input and output layers.\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            hp.Float(\"lr\", min_value = 1e-4, max_value = 1e-2, sampling = \"log\")\n",
    "        ),\n",
    "        loss = 'mse',\n",
    "        metrics = [tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor = 'loss',\n",
    "    patience = 10,\n",
    "    min_delta = 0.001,\n",
    "    restore_best_weights = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "lr_schedule = callbacks.ReduceLROnPlateau(\n",
    "    monitor = 'loss',\n",
    "    patience = 5,\n",
    "    factor = 0.2,\n",
    "    min_lr = 0.001,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir = './logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=1000,\n",
    "    executions_per_trial=3,\n",
    "    overwrite=True,\n",
    "    directory='my_dir',\n",
    "    project_name = 'wine_quality'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train, epochs = 1000, validation_data = (X_val, y_val), callbacks = [early_stopping, lr_schedule, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31671a60cee805c34c73116577b485118ff3a75c458d3004d49632c19702ac60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
